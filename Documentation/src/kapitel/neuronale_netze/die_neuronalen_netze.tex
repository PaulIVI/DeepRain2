\newpage

\section{Die neuronalen Netze}\label{die neuronalen netze}

Die Regenvorhersage mit Hilfe von künstlichen neuronalen Netzen möchten wir mit einem Zitat einführen.
Der Nobelpreisträge Richard P. Feynman ist in einem Gespräch mit einem nicht namentlich erwähnten Laien wobei es um die Existenz fliegender Untertassen geht. Feynman trifft die Aussage, dass es sehr unwahrscheinlich ist, dass es fliegende Untertassen gibt. Der Laie antwortet darauf, dass das sehr unwissenschaftlich sei, worauf Feynman erwiedert:

\begin{quote}{Richard P. Feynman}
''...But that is the way that is scientific. It is scientific only to say what is more likely and what less likely, and not to be proving all the time the possible and impossible.''  
\end{quote}

\noindent Treffend wird in dem Gespräch von Feynman erläutert, dass der wissenschaftliche Weg eine Wahrscheinlichkeit beinhaltet, die Auskunft über das Eintreten eines Ereignisses gibt. Wir werden für die Regenvorhersage also eine probabilistische Aussage treffen. Dies bedeutet insbesondere, dass eine Verteilung für die Regenvorhersage geschätzt wird, durch die eine Aussage über die Regenwahrscheinlichkeit und auch die Wahrscheinlichkeit der Intensität getroffen werden kann.\\

\noindent Bei der Regenvorhersage via KNN bekommt das KNN als Input ein Set von Bildern, woraus ein Feld von Parametern geschätzt wird, wodurch in Kombination mit der zugehörigen Verteilung eine 30-minütige Vorhersage generiert werden kann. Dieser Vorgang ist in der nachfolgenden Abbildung skizziert.


\begin{figure}[htb]
 \centering
 \includegraphics[width=1.0\textwidth,angle=0]{abb/skizzierung_regenvorhersage}
 \caption{Skizzierung der Vorhersage}
\label{fig:Vorhersage_skizze}
\end{figure}

\noindent Wie aus Abbildung \ref{fig:Vorhersage_skizze} hervorgeht, besteht der Ausgang des KNN aus einem Feld von Verteilungen. Diese Verteilungen nehmen wir als unabhängig an.
Die zu schätzende Verteilung ist im vornherein nicht klar, weshalb wir eingangs drei Verteilungen begutachten.
Wir schauen uns die Multinomiale, die Poisson und die negativ Binomial Verteilung an.
Die Poisson und die negativ Binomialverteilung wird allerdings mit der Multinomialen Verteilung gemischt, sodass wir die ''Zero-Inflated negativ Binomial'' bzw. ''Zero-Inflated Poisson'' Verteilung erhalten.
Dies kann so aufgefasst werden, dass wir eine Bernoulli-Verteilung für die binäre Entscheidung über Regen oder kein Regen erhalten. Für den Fall dass Regen vorhergesagt wird, greifen wir auf die Negativ Binomial bzw. Poisson Verteilung zurück.
\\

\subsection{Netzwerk Topologie}

\noindent Wir schauen uns zwei verschiedene Netzwerarchitekturen an. Hierbei passen wir uns den Beschränkungen der uns zur Verfügung stehenden Hardware an.
Uns steht eine Geforce 2060 Super mit 8GB VRAM zur Verfügung. Die Netzwertopologie ist auf die Größe des VRAMS beschränkt, wir werden unser Netzwerk also auf diese Größe beschränken. Die Größe der Eingangsbilder setzen wir auf 96x96 Pixel fest und die Größe der Ausgangsbilder auf 64x64. Die Patches werden um den Pixel, der Konstanz repräsentiert ausgeschnitten.
Alle Netzwerke die wir trainieren haben die Klassifikationsschicht gemeinsam, unterscheiden sich jedoch für die verschiedenen Verteilungen.\\

\begin{figure}[h]
 \centering
\begin{tikzpicture}[inputnode/.style={circle, draw=black!60, fill=green!5,  minimum size=10mm},
input/.style={circle},
outputnode/.style={circle},
]


\node[rectangle, 
dashed, 
draw=sublimedg, 
fill=gray,  
fill opacity=0.2,
very thick, 
minimum width=40em, 
minimum height=20.0em] (duh) at (4, 3) {};

\node[input]            (x0)               at (-3,3)         {};

\node[inputnode]        (inputlayer)       at (3,2)         {};
\node[input]            (x4)               at (3,3)         {$\vdots$};
\node[input]            (x2_3)             at (3.5,3)       {256};
\node[inputnode]        (inputlayer_1)     at (3,4)         {};

\node[outputnode]       (outputlayer)      at (6,2)         {};
\node[inputnode]        (x1)               at (0,5)         {};
\node[input]            (x2)               at (0,3)         {$\vdots$};
\node[input]            (x2_2)             at (0,3.5)       {4096};
\node[inputnode]        (x3)           	   at (0,1)         {};


\node[inputnode]        (x5)               at (6,5)         {};
\node[input]            (x6)               at (6,3)         {$\vdots$};
\node[input]            (x2_1)             at (6,3.5)       {4096 x $n$};
\node[inputnode]        (x7)           	   at (6,1)         {};
%\node[inputnode]        (x9)           	   at (10,3)         {};

\node[cascaded,
		minimum width=4em, 
		minimum height=4em] (x9) at (10,3) {64x64};

\draw[-latex] (x1.east) -- node[above] {} (inputlayer.west);
\draw[-latex] (x3.east) -- node[above] {} (inputlayer.west);
\draw[-latex] (x1.east) -- node[above] {} (inputlayer_1.west);
\draw[-latex] (x3.east) -- node[above] {} (inputlayer_1.west);

\draw[-latex] (inputlayer.east) -- node[above] {} (x5.west);
\draw[-latex] (inputlayer.east) -- node[above] {} (x7.west);

\draw[-latex] (inputlayer_1.east) -- node[above] {} (x5.west);
\draw[-latex] (inputlayer_1.east) -- node[above] {} (x7.west);

\draw[-latex] (x0.east) -- node[above] {Flatten} (x2.west);
\draw[-latex] (x6.east) -- node[above] {64x64x$n$} (x9.west);

\end{tikzpicture}
 \caption{Klassifikationsschicht, $n$ variiert hierbei je nach Verteilung. Für die Zero-Inflated Poisson Verteilung ist $n$ gleich 2, für die Zero-Inflated Negativ Binomial Verteilung ist $n$ gleich 3, und für die Multinomiale Verteilung ist $n$ gleich 7  }
\label{fig:classification_layer}
\end{figure}

\newpage
\noindent Wir verwenden zum einen die Unet-Architektur, wie sie von unseren Vorgängern verwendet wird. Diese Architektur ist in der folgenden Abbildung zu sehen.



\begin{figure}[h]
 \centering
\begin{tikzpicture}[thick,scale=0.5, every node/.style={scale=0.5},inputnode/.style={circle, draw=black!60, fill=green!5,  minimum size=10mm},
input/.style={circle},
outputnode/.style={circle},]
	

	% Schicht 1
	\node[input,rotate=90] (top1_txt) at (0.5,-1) {64x64};
	\node[slimconv] (top1) at (1, 1) {5} ;
	\node[slimconv] (top2) at (2.0, 1) {20} ;
	\node[slimconv] (top3) at (3.0, 1) {20};
	\draw[-stealth, very thick] (top1.east) -- (top2);
	\draw[-stealth, very thick] (top2.east) -- (top3);


	% Schicht 2
	\node[input,rotate=90] (top1_txt) at (2.5,-4.25) {32x32};
	\node[slimconv,
			minimum height=6em] (top1_1) at (3.0, -3.5) {25};
	\node[slimconv,
			minimum width=1.5em, 
			minimum height=6em] (top2_1) at (4.0, -3.5) {25};
	\node[slimconv,
			minimum width=1.5em, 
			minimum height=6em] (top3_1) at (5.125, -3.5) {25};
	\draw[-stealth, very thick] (top1_1.east) -- (top2_1);
	\draw[-stealth, very thick] (top2_1.east) -- (top3_1);
	\draw[-stealth, very thick,red] (top3.south) -- (top1_1.north);


	% Schicht 3
	\node[input,rotate=90] (top1_txt) at (4.5,-6.125) {16x16};
	\node[slimconv,
			minimum width=2em, 
			minimum height=3em] (top1_2) at (5.125, -6.0) {25};
	\node[slimconv,
			minimum width=2.5em, 
			minimum height=3em] (top2_2) at (6.75, -6.0) {25};
	\node[slimconv,
			minimum width=2.5em, 
			minimum height=3em] (top3_2) at (8.5, -6.0) {25};

	\draw[-stealth, very thick] (top1_2.east) -- (top2_2);
	\draw[-stealth, very thick] (top2_2.east) -- (top3_2);
	\draw[-stealth, very thick,red] (top3_1.south) -- (top1_2.north);


	% Schicht 4
	\node[input,rotate=90] (top1_txt) at (7.825,-7.75) {8x8};
	\node[slimconv,
			minimum width=2em, 
			minimum height=2em] (top1_3) at (8.5, -7.75) {25};
	\node[slimconv,
			minimum width=3.5em, 
			minimum height=2em] (top2_3) at (10.25, -7.75) {25};
	\node[slimconv,
			minimum width=3.5em, 
			minimum height=2em] (top3_3) at (12.25, -7.75) {25};

	\draw[-stealth, very thick] (top1_3.east) -- (top2_3);
	\draw[-stealth, very thick] (top2_3.east) -- (top3_3);
	\draw[-stealth, very thick,red] (top3_2.south) -- (top1_3.north);



	% Schicht 5
	\node[input,rotate=90] (top1_txt) at (11.25,-9.25) {4x4};
	\node[slimconv,
			minimum width=3.5em, 
			minimum height=1.5em] (top1_4) at (12.25, -9.25) {25};
	\node[slimconv,
			minimum width=3.5em, 
			minimum height=1.5em] (top3_4) at (16.75, -9.25) {25};

	%\draw[-stealth, very thick] (top1_4.east) -- (top3_4) ;
	\draw[-stealth, very thick] (top1_4.east) -- (top3_4);
	\draw[-stealth, very thick,red] (top3_3.south) -- (top1_4.north);

	% Schicht 4 ( UP )

	\node[slimconv,
			minimum width=2em,
			fill=white,
			minimum height=2em] (top4_30) at (15.6, -7.75) {};
	\node[slimconv,
			minimum width=3.5em, 
			minimum height=2em] (top4_31) at (16.75, -7.75) {50};
	\node[slimconv,
			minimum width=3.5em, 
			minimum height=2em] (top5_3) at (19.25, -7.75) {50};
	\node[slimconv,
			minimum width=3.5em, 
			minimum height=2em] (top6_3) at (21.25, -7.75) {50};


	
	\draw[-stealth, very thick,opacity=0.5] (top3_3.east) -- (top4_30.west);
	\draw[-stealth, very thick] (top4_31.east) -- (top5_3);
	\draw[-stealth, very thick] (top5_3.east) -- (top6_3);
	\draw[-stealth, very thick,green] (top3_4.north) -- (top4_31.south);


		% Schicht 3 (UP)
	\node[slimconv,
			minimum width=2em,
			fill=white,
			minimum height=3em] (top4_20) at (20.125, -6.0) {};
	\node[slimconv,
			minimum width=3.5em, 
			minimum height=3em] (top4_21) at (21.25, -6.0) {75};
	\node[slimconv,
			minimum width=2.5em, 
			minimum height=3em] (top5_2) at (23.25, -6.0) {75};
	\node[slimconv,
			minimum width=2.5em, 
			minimum height=3em] (top6_2) at (25, -6.0) {75};


	\draw[-stealth, very thick,opacity=0.5] (top3_2.east) -- (top4_20.west);
	\draw[-stealth, very thick] (top4_21.east) -- (top5_2);
	\draw[-stealth, very thick] (top5_2.east) -- (top6_2);
	\draw[-stealth, very thick,green] (top6_3.north) -- (top4_21.south);

	% Schicht 2
	\node[slimconv,
		minimum width=1.5em,
		fill=white,
		minimum height=6em] (top4_10) at (24.18, -3.5) {};
	\node[slimconv,
			minimum width=2.5em,
			minimum height=6em] (top4_11) at (25.0, -3.5) {100};
	\node[slimconv,
			minimum width=1.5em, 
			minimum height=6em] (top5_1) at (26.5, -3.5) {100};
	\node[slimconv,
			minimum width=1.5em, 
			minimum height=6em] (top6_1) at (27.75, -3.5) {100};

	\draw[-stealth, very thick,opacity=0.5] (top3_1.east) -- (top4_10.west);
	\draw[-stealth, very thick] (top4_11.east) -- (top5_1);
	\draw[-stealth, very thick] (top5_1.east) -- (top6_1);
	\draw[-stealth, very thick,green] (top6_2.north) -- (top4_11.south);



	
	\node[slimconv,fill=white,] (top4_0) at (27.125, 1) {};
	\node[slimconv] (top4_1) at (27.75, 1) {120};
	\node[slimconv] (top5) at (29, 1) {n};
%	\node[slimconv] (top6) at (29.75, 1) {};

	\draw[-stealth, very thick,opacity=0.5] (top3.east) -- (top4_0.west);
	\draw[-stealth, very thick] (top4_1.east) -- (top5);
%	\draw[-stealth, very thick] (top5.east) -- (top6);
	\draw[-stealth, very thick,green] (top6_1.north) -- (top4_1.south);





	\node[input]        (A1)       at (27.75, -8)         {};
	\node[input]        (A2)       at (27.75, -9.5)         {};
	\draw[-stealth, very thick,green] (A2.north) -- node[right,opacity=1.0,black] {Up-Sampling} (A1.south);


	\node[input]        (A3)       at (27.75, -6)         {};
	\node[input]        (A4)       at (27.75, -7.5)         {};
	\draw[-stealth, very thick,red] (A3.south) -- node[right,opacity=1.0,black] {Pooling} (A4.north);



	\node[input]        (A5)       at (27.75, -7.75)         {};
	\node[input]        (A6)       at (30.0, -7.75)         {Conv 3x3};
	\draw[-stealth, very thick,black] (A5.east) -- (A6.west);



	\node[input]        (A7)       at (25.75, -7.75)         {};
	\node[input]        (A8)       at (27.25, -7.75)         {};
	\draw[-stealth, very thick,gray] (A7.east) --node[above,opacity=1.0,black]{Skipping} (A8.west);


\node[rectangle, 
dashed, 
draw=sublimedg, 
fill=gray,  
fill opacity=0.4,
very thick, 
minimum width=5em, 
minimum height=5em] (kl) at (31, 1) {Kl-Schicht};
	\draw[-stealth, very thick] (top5.east) -- (kl.west);

\end{tikzpicture}
 \caption{Abgespekte Version des Unets  }
\label{fig:Unet1}
\end{figure}

\noindent Die abgespekte Version des Unets hat im Gegensatz zur originalen Architektur wesentlich weniger Gewichte.
Hier sind es ca 10 000 trainierbare Gewichte. Hierbei liegt der Hauptteil der Gewichte in der Klassifikationsschicht (Kl-Schicht in Bild \ref{fig:Unet1}).\\


\noindent Eine weitere Architektur die wir begutachten ist ein Mix aus Inception-Layer und CNN-LSTM-Layer. Die Regenvorhersage in unserem Setup wird für gewöhnlich auch als Next-Frame prediction bezeichnet. Aus einem Set aufeinander folgender eingehender Bilder wird eine plausible vorhersage für das nächste Bild erzeugt.


%\begin{wrapfigure}{R}{0.5\textwidth}
\begin{figure}[h]
 \centering
\begin{tikzpicture}[thick,scale=0.8, every node/.style={scale=0.8}]

\node[rectangle, 
dashed, 
draw=sublimedg, 
fill=gray,  
fill opacity=0.2,
very thick, 
minimum width=20em, 
minimum height=21.0em] (duh) at (2.75, -7.25) {};


\node[cascaded,
		minimum width=4em, 
		minimum height=4em] (conv1) at (1, -10) {1x1};

\node[cascaded,
		minimum width=4em, 
		minimum height=4em] (conv3x3) at (3, -10) {3x3};

\node[cascaded,
		minimum width=4em, 
		minimum height=4em] (conv3x3_1) at (5, -10) {3x3};

\node[cascaded,
		minimum width=4em, 
		minimum height=4em] (conv3x3_2) at (4.5, -7.5) {3x3};


\node[cascaded,
		minimum width=4em, 
		minimum height=4em] (conv3x3_2) at (3, -5.0) {stack};

%\draw[very thick, sublimedg, dashed] 

\node[rectangle, dashed, draw=sublimedg, fill=white,  fill opacity=0.5,very thick, minimum width=1.0em, minimum height=1.0em] (R0) at (1.5, -9.5) {};

\node[rectangle, dashed, draw=sublimedg, fill=white,  fill opacity=0.5,very thick, minimum width=1.0em, minimum height=1.0em] (R1) at (3.5, -9.5) {};

\node[rectangle, dashed, draw=sublimedg, fill=white,  fill opacity=0.5,very thick, minimum width=1.0em, minimum height=1.0em] (R2) at (5.5, -9.5) {};

\node[rectangle, dashed, draw=sublimedg, fill=white,  fill opacity=0.5,very thick, minimum width=1.0em, minimum height=1.0em] (R3) at (5.0, -7.0) {};


\node[circle, inner sep = 0.1em, fill=sublimedg] (C0) at (2.5, -5.5) {};
\node[circle, inner sep = 0.1em, fill=sublimedg] (C1) at (2.75, -5.5) {};
\node[circle, inner sep = 0.1em, fill=sublimedg] (C2) at (5, -8.0) {};
\node[circle, inner sep = 0.1em, fill=sublimedg] (C3) at (3, -5.5) {};



\draw[very thick, sublimedg, dashed] (R0.north east) -- (C0);
\draw[very thick, sublimedg, dashed] (R0.north west) -- (C0);
\draw[very thick, sublimedg, dashed] (R0.south west) -- (C0);
\draw[very thick, sublimedg, dashed] (R0.south east) -- (C0);

\draw[very thick, sublimedg, dashed] (R1.north east) -- (C1);
\draw[very thick, sublimedg, dashed] (R1.north west) -- (C1);
\draw[very thick, sublimedg, dashed] (R1.south west) -- (C1);
\draw[very thick, sublimedg, dashed] (R1.south east) -- (C1);


\draw[very thick, sublimedg, dashed] (R2.north east) -- (C2);
\draw[very thick, sublimedg, dashed] (R2.north west) -- (C2);
\draw[very thick, sublimedg, dashed] (R2.south west) -- (C2);
\draw[very thick, sublimedg, dashed] (R2.south east) -- (C2);

\draw[very thick, sublimedg, dashed] (R3.north east) -- (C3);
\draw[very thick, sublimedg, dashed] (R3.north west) -- (C3);
\draw[very thick, sublimedg, dashed] (R3.south west) -- (C3);
\draw[very thick, sublimedg, dashed] (R3.south east) -- (C3);





%\draw[very thick, sublimedg, dashed] (R1C.north east) -- (C1C);
\end{tikzpicture}
 \caption{Aufbau der von uns verwendeten Inception Layer, angelehnt an die inception Layer im Paper }
\label{fig:inception v2}
\end{figure}

\newpage

\noindent In der nachfolgenden Abbildung ist die von uns verwendete Architektur zu sehen. Diese Architektur ist angelehnt an die Inception-LSTM Layer des Papers ''Inception-inspired LSTM for Next-frame Video Prediction'' von \cite{hosseini2019inceptioninspired}. Wie Eingangs erwähnt beschränkt die Hardware (und auch die größe der Bilder) unsere Architektur und aus diesem Grund werden nicht mehr LSTM-Layer gestackt, wie im Paper beschrieben.
\begin{figure}[h]
 \centering
\begin{tikzpicture}[thick,scale=0.8, every node/.style={scale=0.8},->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,main node/.style={circle,fill=blue!30,draw,
  font=\sffamily\Large\bfseries,minimum size=15mm}]



\node[rectangle, 
dashed, 
draw=sublimedg, 
fill=gray,  
fill opacity=0.4,
very thick, 
minimum width=5em, 
minimum height=5em] (inc1) at (1, 1) {Inception Layer};
	

\node[main node,font=\sffamily\small] (LSTM_1) at (6,1) {CNN-LSTM}
  (LSTM_1) edge [loop right] (LSTM_1);

\node[main node,font=\sffamily\small] (LSTM_2) at (6,-2.5) {CNN-LSTM}
  (LSTM_2) edge [loop right] (LSTM_2);

\node[main node,font=\sffamily\small] (LSTM_3) at (6,-6) {CNN-LSTM};
  

\draw[very thick, sublimedg] (LSTM_1.south) -- (LSTM_2.north);
\draw[very thick, sublimedg] (LSTM_2.south) -- (LSTM_3.north);

\draw[very thick, sublimedg] (inc1.east) -- (LSTM_1.west);




\node[rectangle, 
dashed, 
draw=sublimedg, 
fill=gray,  
fill opacity=0.4,
very thick, 
minimum width=5em, 
minimum height=5em] (inc2) at (1, -2.5) {Inception Layer};



\node[rectangle, 
dashed, 
draw=sublimedg, 
fill=gray,  
fill opacity=0.4,
very thick, 
minimum width=5em, 
minimum height=5em] (inc3) at (1, -6) {Inception Layer};


\node[rectangle, 
dashed, 
draw=sublimedg, 
fill=gray,  
fill opacity=0.4,
very thick, 
minimum width=5em, 
minimum height=5em] (inc4) at (1, -9.5) {Inception Layer};



\node[rectangle, 
dashed, 
draw=sublimedg, 
fill=gray,  
fill opacity=0.4,
very thick, 
minimum width=5em, 
minimum height=5em] (inc5) at (3, -13) {Inception Layer};

\draw[very thick, sublimedg] (inc1.south) -- (inc2.north);
\draw[very thick, sublimedg] (inc2.south) -- (inc3.north);
\draw[very thick, sublimedg] (inc3.south) -- (inc4.north);
\draw[very thick, sublimedg] (inc4.south) -- (inc5.north);
\draw[very thick, sublimedg] (LSTM_3.south) -- (inc5.north);


\node[cascaded,
		minimum width=4em, 
		minimum height=4em] (conv1) at (6.5, -13) {7x7};


\node[cascaded,
		minimum width=4em, 
		minimum height=4em] (conv2) at (9.5, -13) {7x7};



\node[rectangle, 
dashed, 
draw=sublimedg, 
fill=gray,  
fill opacity=0.4,
very thick, 
minimum width=5em, 
minimum height=5em] (kl) at (12.5, -13) {Kl-Schicht};



\draw[very thick, sublimedg] (inc5.east) -- (conv1.west);
\draw[very thick, sublimedg] (conv1.east) -- (conv2.west);
\draw[very thick, sublimedg] (conv2.east) -- (kl.west);

\end{tikzpicture}
 \caption{LSTM Architektur}
\label{fig:LSTM-CNN}
\end{figure}

\noindent Unsere Architektur unterscheidet sich jedoch von der im Paper vorgestellten Architektur insofern, als dass die Inception-Layer nicht in das LSTM-Modul eingebaut sind.
Wir verwenden hierbei herkömmliche Convolution-LSTM Layer. Die Inception-Layer sind hierbei lediglich vor oder nach den Convolution-LSTM Layern zu finden.
Der Vorteil von Inception-Layer ist, dass diese in die Breite und nicht so sehr in die Tiefe gehen. Das hat zum Vorteil, dass beim aktualisieren der Gewichte der Gradient nicht so weit in das Netzwerk durchgereicht werden muss. Dadurch soll das Auftreten des vanishing Gradients vermindert werden und das aktualisieren der Gewichte bzw. das lernen verbessert werden.\\

\subsection{Training}
Beide Architekturen werden mindestens 35 Epochen trainiert, wobei die Architektur mit den LSTM-Layern 1:30 Stunden benötigt, das Unet hingegen benötigt ca. 10 Minuten pro Epoche.
Für das Training nutzen wir alle Daten der Jahre 2008 bis einschließlich 2017. Hierbei werden 75\% der Daten für das Training und 25\% der Daten für das Testset verwendet.
Da der Hauptteil der Regendaten Null ist, also kein Regen vorhanden ist, liegt der Mittelwert der Daten schon nahe Null. Das bedeutet, dass der Mittelwert nicht (wie üblich) auf Null normiert wird.
Die Standardabweichung ist ebenfalls nahe Null, weshalb die Standardabweichung der Daten nicht auf Eins normieren (Dies würde zur Folge haben, dass Werte wesentlich größer als 1 sein können). 
Die Eingangsdaten werden allerdings auf den Bereich zwischen Null und Eins normiert.\\

\noindent Für den Ausgang der Netzwerke beschränkten wir uns auf einen 64x64 großen Pixelbereich, bei dem Konstanz in der Mitte liegt. Die Eingangsbilder bestehen aus einem 96x96 großen Pixelbereich um Konstanz. Regenfreie Bilder werden aussortiert, da diese Daten keinerlei Informationen beinhalten und das vorhandene Klassenungleichgewicht verstärken. Als Regularisierungsmaßnahme werden die Trainingsdaten pro Epoche um wenige Pixel verschoben, was zur Folge hat, dass das Netzwerk in jeder Epoche auf etwas unterschiedliche Daten trainiert. Als Kostenfunktion verwenden wir die Negative Loglikelihood.
\\

\noindent In der Nachfolgenden Abbildung sind die Trainingskurve für die beiden Architekturen zu sehen. Die hierfür Verwendete Verteilung ist die Zero-Inflated negative Binomial Verteilung.

\begin{figure}[htb]
 \centering
 \includegraphics[width=0.9\textwidth,angle=0]{abb/Loss_zinfBinom.png}
 \caption{Trainingskurven für Zero-Inflated negativ Binomial verteilung}
\label{fig:Inception-Conv-LSTM}
\end{figure}

\noindent Zu sehen ist, dass die Unet-Architektur schlechtere Performance als die LSTM-Architektur liefert. In dieser Abbildung scheint der Overfitting bereich noch nicht erreicht worden zu sein.
Tatsächlich verbessern sich beide Architekturen noch marginal nach weiteren Epochen, aus Darstellungsgründen wurde die X-Achse auf 35 Epochen beschnitten.\\

\noindent Zusätzlich zur Zero-Inflated negative Binomial Verteilung wurden beide Architekturen mit einer weiteren Verteilung trainiert. Hierfür verwenden wir die Multinomiale Verteilung, woebei wir die Daten in Sieben Klassen einteilen. Die geschieht durch logarithmisches skalieren der Daten. Dadurch soll zusätzlich dem Klassenungleichgewicht entgegengesteuert werden (Regenwerte im höheren Bereich kommen seltener vor). 

\begin{figure}[htb]
 \centering
 \includegraphics[width=0.9\textwidth,angle=0]{abb/Categorical.png}
 \caption{Multinomiale Verteilung}
\label{fig:multinomialeVerteilung}
\end{figure}

\noindent In der obigen Abbildung sind die Trainingskurven der beiden Architekturen zu sehen. Auch hier ist zu sehen, dass die LSTM-Architektur der Unet-Architektur überlegen ist und dies eine bessere Performance liefert. Vergleicht man die Trainingskurven für beide Verteilungen sieht man, dass der NLL für die Multinomiale Verteilung etwa die Hälfte der Zero-Inflated negative Binomialverteilung entspricht.

\newpage
\subsection{Auswertung}



\begin{figure}[h]
\begin{tabular}{lllllll}
\includegraphics[width=20mm]{abb/prediction/280_maxCont}&
\includegraphics[width=20mm]{abb/prediction/281_maxCont}&
\includegraphics[width=20mm]{abb/prediction/282_maxCont}&
\includegraphics[width=20mm]{abb/prediction/283_maxCont}&
\includegraphics[width=20mm]{abb/prediction/284_maxCont}&
\includegraphics[width=20mm]{abb/prediction/285_maxCont}&
\includegraphics[width=20mm]{abb/prediction/286_maxCont}
\end{tabular}
\caption{Vorhersage in Abständen von fünf Minuten für die Zero-Inflated negative Binomialverteilung, erste Zeile entspricht dem momentanen Regen. Zeile zwei entspricht dem Regen in 30 Minuten. Zeile drei ist die 30 Minuten Vorhersage des Unets. Die letzte Zeile ist die 30 Minuten Vorhersage der LSTM-Architektur. Die Bilder sind Kontrastmaximiert dargestellt. \label{fig:anomerz}}
\end{figure}



\begin{figure}[h]
\begin{tabular}{cc}
\includegraphics[width=70mm]{abb/ROC_ZINFBINOM.png}&
\includegraphics[width=70mm]{abb/ROC_Categorical.png}
\end{tabular}
\caption{ROC/AUR für beide Architekturen und beide Verteilungen. Links für die Zero-Inflated negativ Binomialverteilung und rechts für die Multinomialeverteilung.
Die Auserwertung erfolgt für 20 verschiedene Thresholds. \label{fig:anomerz}}
\end{figure}





\newpage
