\documentclass[oneside]{htwg-report}

\include{header/header}

\addbibresource{./bib/report.bib}

\begin{document}

\pagenumbering{gobble}

%% 'reporttype' add background elements to the cover / front page
%% possible values are:
%% bachelor	--> B S C
%% master	--> M S C
%% other		--> none
\reporttype{master}

\reporttypetext{Teamproject (Master 3. semester)}

\newcommand{\verfasserA}{Simon Christofzik}
\newcommand{\verfasserB}{Paul Sutter}
\newcommand{\verfasserC}{Till Reitlinger}
\newcommand{\thema}{DeepRain: Rain forecast with neural networks and the visualization of these in an App}
\newcommand{\hoschschule}{HTWG Konstanz - University of Applied Sciences}
\newcommand{\institut}{HTWG Konstanz - Institute for Optical Systems}
\newcommand{\prueferA}{Prof. Dr. Oliver DÃ¼rr}


\title[Teamprojektthema]{\thema}

\doclocation{Konstanz}
\docdate{10. September 2020}

\makecover[]

\input{extended_abstract}
\twocolumn
\section*{Introduction}
    \begin{sloppypar}
        \tolerance 9999
        Even today, rain forecasts are still very computationally complex and relatively inaccurate. 
        Therefore, it may make sense to make such predictions with the help of neural networks. 
        These do not require as much computing power and can recognize a pattern in the often chaotic data even without complex physical models.
    \end{sloppypar}

\section*{Data}\label{data}
\begin{sloppypar}
Images must be generated from the binary radar data provided by the German Weather Service (DWD) in five-minute resolution.
To convert the radar data into an image, each radar data point must be transformed into a pixel color value.
In order to achieve a suitable transformation, the radar data of June 2016 were inspected exemplarily (figure \ref{fig:Radardatapoints_of_June_2016}). 
\end{sloppypar}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth,angle=0]{../abb/Radardatapoints_of_June_2016.png}
    \caption[Datenaufbereitung]{Frequency distribution of rain data}
    \label{fig:Radardatapoints_of_June_2016}
\end{figure}
\begin{sloppypar}
All occurring values and their frequency are plotted. 
Two outliers stand out in figure \ref{fig:Radardatapoints_of_June_2016}, which occur much more frequently than the other values. 
The value -9999 indicates that no data is available and the second outlier is at 0, which stands for no rain. 
In Figure \ref{fig:Radardatapoints_of_June_2016_larger0_99percentile} the two outliers are filtered, because the relevant information is contained in the remaining data points.
In addition, the 99\% percentile illustrates the frequency distribution. 
\end{sloppypar}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth,angle=0]{../abb/Radardatapoints_of_June_2016_larger0_99percentile.png}
    \caption[Datenaufbereitung]{Frequency distribution of rain data with 99\% percentile}
    \label{fig:Radardatapoints_of_June_2016_larger0_99percentile}
\end{figure}
\begin{sloppypar}
The 99\% percentile contains 138 different values, each of which is assigned a color value.
The remaining data is transformed linearly to the remaining range of values.
This results in rounding and maximum value errors. These errors are bearable, because it is more important to map all data points.
Furthermore the resolution of the different rainfall intensities is still higher than the human perception.
Reconstructing the radar data from the PNG data results in the plot shown in figure \ref{fig:Radardatapoints_of_June_2016_RecoveredData}.
\end{sloppypar}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth,angle=0]{../abb/Radardatapoints_of_June_2016_RecoveredData.png}
    \caption[Datenaufbereitung]{Comparison between output distribution and distribution from recovered data}
    \label{fig:Radardatapoints_of_June_2016_RecoveredData}
\end{figure}






\section*{Data Validation}\label{data validation}
\begin{sloppypar}
To validate the quality of the data as well as the correct position of the constant pixel, the radar data were compared with the weather station in Konstanz.
The radar data of one week from all over Germany were correlated with the weather station.
The correlation coefficient reached its maximum around Konstanz.
There are deviations from radar data to rainfall compensation, especially with small rainfall values, but these are caused by inaccuracies of the weather station itself (figure \ref{fig:radar_station_daten_vergleich_June}).
The correlation is significant, the position of Konstanz could be validated and the data quality is suitable for a rain forecast.
\end{sloppypar}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth,angle=0]{../abb/radar_station_daten_vergleich_June.png}
    \caption[Datenaufbereitung]{Comparison between Radar and Weatherstation in June 2016}
    \label{fig:radar_station_daten_vergleich_June}
\end{figure}

\section*{DeepRain Application}
    \begin{sloppypar}
        The DeepRain App was developed with Flutter. 
        Firebase is used for the user administration, the forecast image transmission and the sending of rain warnings (push notifications). 
        The App basically consists of three screens. One screen is the settings. 
        In the settings you can change the region, configure the rain warning function and some other settings.
        On the Rain Map screen the current and future rain situation is displayed using the forecast PNGs. With a slider you can display the rain situation at different times.
        On the Prediction List screen the current and future rain situation is displayed quantitatively. The rain values are read from the respective pixel of the region in the forecast image.   
    \end{sloppypar}
\subsection*{Calculate rain intense}
    \begin{sloppypar}
        To display the rain intensity of a certain region in a list, the color value of the pixel belonging to the region must be read out. 
        To calculate the pixel of the region in the forecast image, an algorithm was developed, which finds the corresponding pixel in the forecast image with the help of a latitude and longitude coordinate. 
        This is only possible because the latitude and longitude coordinates of each pixel are known. 
        The algorithm starts at any pixel, checks the current distance to the target coordinates and the distance of the neighboring pixels to the target coordinates. 
        If an neighboring pixel has a smaller distance, the pixel is updated. If no more improvement can be achieved, the correct pixel is found. 
    \end{sloppypar}


\section*{Conclusion and Future Work}
\begin{sloppypar}
Both neural networks exceed the performance of the baseline. 
Nevertheless an improvement of the nets is still desirable. 
The pipeline from the server to the app runs stable and newly developed networks can be integrated with minimal effort. 
\end{sloppypar}

\end{document}

